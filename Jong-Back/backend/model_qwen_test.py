# model_qwen.py
from typing import Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

_MODEL_NAME = "Qwen/Qwen2-0.5B-Instruct"

class QwenGenerator:
    """
    Qwen-0.5B í…ìŠ¤íŠ¸ ìƒì„±ê¸°.
    - ì²« í˜¸ì¶œ ì‹œ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ(ì‹±ê¸€í†¤).
    - thread-safeë¥¼ ìœ„í•´ .generateì—ì„œ torch.no_grad() ì‚¬ìš©.
    """
    _instance: Optional["QwenGenerator"] = None

    def __init__(self):
        # device / dtype ì„¤ì •
        self.device_map = "auto"  # GPU ìˆìœ¼ë©´ ìë™ í• ë‹¹, ì—†ìœ¼ë©´ CPU
        self.torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

        # í† í¬ë‚˜ì´ì €/ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            _MODEL_NAME,
            trust_remote_code=True,
            use_fast=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            _MODEL_NAME,
            trust_remote_code=True,
            torch_dtype=self.torch_dtype,
            device_map=self.device_map
        )

        # pad í† í° ì„¤ì • (ì—†ì„ ê²½ìš° ëŒ€ë¹„)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        if self.model.config.pad_token_id is None:
            self.model.config.pad_token_id = self.tokenizer.pad_token_id

    @classmethod
    def get(cls) -> "QwenGenerator":
        if cls._instance is None:
            cls._instance = QwenGenerator()
        return cls._instance

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 128,
        temperature: float = 0.7,
        top_p: float = 0.95
    ) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        generated = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        if generated.startswith(prompt):
            return generated[len(prompt):].lstrip()
        return generated


# âœ… í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ì¶”ê°€
if __name__ == "__main__":
    qwen = QwenGenerator.get()
    user_prompt = input("ğŸ‘‰ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    result = qwen.generate(user_prompt, max_new_tokens=100)
    print("\nğŸ¤– Qwen ì‘ë‹µ:")
    print(result)